# ğŸ¤– AI Conversational Assistant â€” LLM-Powered Chatbot

A prototype of an intelligent conversational agent built with **Next.js**, **Python**, and **Ollama**. This project served as the foundation for a production chatbot deployed at LPEE (Laboratoire Public d'Essais et d'Ã‰tudes), integrated with SharePoint for internal document querying.

---

## âœ¨ Features

- ğŸ’¬ **Conversational AI** â€” Natural language interaction powered by a local LLM via Ollama
- ğŸ” **JWT Authentication** â€” Secure user authentication and role-based access control
- ğŸ’¾ **Conversation Persistence** â€” Full conversation history saved and retrievable per user
- ğŸ“„ **Multi-file Document Search** â€” Query across multiple internal documents in natural language
- ğŸ–¼ï¸ **Image Analysis** â€” LLM-powered image understanding capabilities
- ğŸ”— **REST API Backend** â€” Secure Python API handling all AI interactions

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| Frontend | Next.js, TypeScript, React |
| Backend | Python, Flask |
| AI / LLM | Ollama (local LLM inference) |
| Auth | JWT (JSON Web Tokens) |
| Styling | Tailwind CSS, shadcn/ui |

---

## ğŸ“ Project Structure

```
chatbot/
â”œâ”€â”€ app/                  # Next.js app router pages
â”œâ”€â”€ components/           # Reusable UI components
â”œâ”€â”€ lib/                  # Utility functions
â”œâ”€â”€ public/               # Static assets
â”œâ”€â”€ styles/               # Global styles
â”œâ”€â”€ app.py                # Python Flask backend
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ next.config.mjs       # Next.js configuration
```

---

## ğŸš€ Getting Started

### Prerequisites

- Node.js 18+
- Python 3.10+
- [Ollama](https://ollama.ai/) installed and running locally
- pnpm

### Backend Setup

```bash
# Install Python dependencies
pip install -r requirements.txt

# Start the Python backend
python app.py
```

### Frontend Setup

```bash
# Install dependencies
pnpm install

# Start the development server
pnpm dev
```

Then open your browser at `http://localhost:3000`

---

## ğŸ§  How It Works

1. User sends a message through the chat interface
2. The Next.js frontend sends the request to the Python REST API
3. The API authenticates the user via JWT and checks role permissions
4. The request is forwarded to the local LLM (via Ollama) for processing
5. The LLM response is returned, and the conversation is persisted
6. The response is displayed in the chat interface

---

## ğŸ‘¨â€ğŸ’» Author

**Mohammed Elyaakoubi**  
---

## ğŸ“„ License

This project is a prototype developed for research and educational purposes.
